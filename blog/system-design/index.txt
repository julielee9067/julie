2:I[3422,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","269","static/chunks/269-a28aad18182cd41e.js","254","static/chunks/app/blog/%5Bcategory%5D/page-6078f51c25971824.js"],"default"]
6:I[4707,[],""]
8:I[6423,[],""]
9:I[3483,["648","static/chunks/648-3ae006cfe07c9d94.js","768","static/chunks/app/blog/layout-a0ac16c7cad7b2d1.js"],"default",1]
a:I[5495,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","185","static/chunks/app/layout-2f9a78561536bd6f.js"],"ThemeProvider"]
b:I[4491,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","185","static/chunks/app/layout-2f9a78561536bd6f.js"],"LanguageProvider"]
c:I[1890,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","185","static/chunks/app/layout-2f9a78561536bd6f.js"],"Header"]
3:T3cad,
## 개요
시스템을 설계할 때 종종 **많은 데이터 중에서 원하는 정보를 찾아내는** 작업이 필요할 때가 있습니다.
이 때, 일반적인 데이터베이스로 충분할 때도 있지만 데이터가 많거나 검색 조건이 복잡할 경우엔 전용 검색 엔진이 필요하기도 합니다.

Elasticsearch는 그 중 대표적인 **검색 엔진**으로 **정렬, 필터링, 순위 매김, 페이징** 등 다양한 기능을 제공합니다.

또한, 내부 동작 원리를 이해한다면 **분산 시스템 설계, 데이터 구조 최적화, immutable** 등과 같은 개념을 배울 수도 있습니다.

## 기본 개념
Elasticsearch의 핵심 구성 요소는 **문서, 인덱스, 매핑, 필드** 입니다.

1. **문서(Document)**

    **문서**는 **검색 대상이 되는 데이터의 최소 단위**입니다. 
    
    반드시 웹 페이지나 글일 필요는 없으며 단순한 **JSON 객체**로 표현할 수 있습니다.
    예를 들어, 영화 데이터베이스를 구축한다고 가정하면 아래와 같이 하나의 문서가 영화 한 편의 정보를 담습니다.
    ```
    {
      "id": "MOV001",
      "title": "기생충",
      "director": "봉준호",
      "rating": 8.6,
      "releaseDate": "2025-01-01T00:00:00.000Z"
    }
    ```

2. **인덱스(Index)**

    인덱스는 **여러 문서들을 논리적으로 모아둔 컨테이너**입니다.
    
    **데이터베이스의 테이블**과 유사한 개념으로, 예를 들면 `movies`라는 인덱스에 영화 정보들이 모두 저장됩니다.

3. **매핑(Mapping)**

    **매핑**은 **인덱스에 저장될 데이터의 구조(스키마)를 정의**합니다.
     
    즉, 각 문서에 포함된 필드(제목, 감독, 평점, 개봉일 등)의 데이터 타입과 검색 방법을 미리 지정합니다.
    예를 들어, 아래와 같은 매핑을 설정하면 `id`는 정확한 값 검색이 가능한 `keyword` type, `title`은 텍스트 분석을 거쳐 검색할 수 있는 `text` type 등으로 지정할 수 있습니다.

    단, **매핑에 너무 많은 필드를 포함**시키면 실제 검색에 사용하지 않는 데이터까지 인덱싱되어 **클러스터 메모리 오버헤드가 증가**할 수 있으므로 주의해야 합니다.

    ```
    {
      "properties": {
        "id": { "type": "keyword" },
        "title": { "type": "text" },
        "director": { "type": "text" },
        "rating": { "type": "float" },
        "releaseDate": { "type": "date" }
      }
    }
    ```

## 사용법
Elasticsearch는 **REST API**를 통해 인덱스 생성, 데이터 저장, 검색 등을 수행할 수 있습니다.

자세한 사용법은 엘라스틱서치 공식 문서에서 쉽게 찾아보실 수 있습니다.

## 동작 원리
Elasticsearch는 아주 빠른 검색을 도와주는 프로그램인 **Apache Lucene** 위에서 동작하는 여러 작업들을 조율하는 상위 시스템이라고 볼 수 있습니다.

쉽게 말하면, **Lucene**은 영화 제목이나 감독 같은 **정보를 빠르게 찾을 수 있도록 도와주는 엔진(검색 도구)** 이고, **Elasticsearch**는 이 **Lucene 엔진을 여러 대의 노드에서 동시에 사용할 수 있게 관리해주는 역할**을 합니다.

Elasticsearch 클러스터는 여러 종류의 **노드**로 이루어져 있습니다.
각 노드는 영화 데이터를 저장하거나 검색 요청을 받아 처리하는 등 서로 다른 역할을 맡습니다.

- **마스터 노드**는 전체 클러스터를 관리하며 **새로운 노드를 클러스터에 추가**하거나 **불필요한 노드를 제거하는 등**의 클러스터 수준의 작업을 합니다.

- **데이터 노드**는 **실제로 데이터를 저장하는 역할**을 합니다.
여기서 저장된 데이터는 실제 영화 저장뿐만 아니라, 검색을 빠르게 하기 위해 미리 만들어 둔 **Lucene 인덱스도 포함**됩니다.

- **코디네이팅 노드**는 사용자로부터 "영화 '인셉션' 같은 걸 찾아줘" 라는 요청을 받고, **어느 데이터 노드에 그 작업을 맡길지 결정**한 후 각 노드로부터 받은 결과를 모아 다시 **사용자에게 전달하는 역할**을 합니다.

- **인제스트 노드**는 새로운 데이터가 들어올 때 이를 미리 처리하여 **인덱싱을 준비**합니다.

- **머신러닝 노드**는 영화 추천이나 분석 등 복잡한 작업을 따로 처리합니다.

#### 예시
예를 들어, 영화 스트리밍 사이트에서 사용자가 새로운 영화를 등록한다고 가정해 보겠습니다.

1. 사용자가 영화 데이터를 **인제스트 노드**에 보내면, 인제스트 노드가 먼저 데이터를 정리한 후 **코디네이팅 노드**에 전달합니다.

2. 코디네이팅 노드는 "어떤 노드가 이 데이터를 저장하고 검색하기에 좋을까?"를 판단해서 적절한 **데이터 노드**에 그 일을 맡깁니다.

3. 데이터 노드는 실제로 **영화 데이터를 저장**하면서, 나중에 검색할 때 빠르게 찾을 수 있도록 미리 만들어둔 **Lucene 인덱스에 이 정보를 반영**합니다.

4. **코디네이팅 노드**는 여러 데이터 노드로부터 받은 결과를 모아 사용자에게 보여줍니다.

#### 데이터 노드의 구조
**데이터 노드**는 영화의 원본 정보(예: 영화의 상세 설명 등)를 **그대로 보관**하는 동시에, 검색 속도를 높이기 위해 **Lucene 인덱스라는 별도의 자료 구조**를 만듭니다.
이를 쉽게 비유하자면 **영화에 대한 모든 상세 정보를 보관하는 책장**과, 그 **책장에서 원하는 영화를 빠르게 찾을 수 있도록 정리해둔 목차**가 따로 있는 것과 같습니다.

데이터 노드는 검색 요청이 들어오면 두 단계로 처리합니다.

1. **Query 단계**: 미리 만들어둔 인덱스(목차)를 이용해 관련 문서들을 빠르게 찾아냅니다.

2. **Fetch 단계**: 찾은 문서의 ID나 필요한 데이터만 선택적으로 가져옵니다.

이상적인 쿼리는 **원본 데이터를 전혀 읽지 않고 인덱스에 포함된 정보만으로 응답**을 줄 수 있는 경우입니다.

데이터 노드에 저장되는 인덱스는 여러개의 **샤드**와 그 **복제본**으로 구성되어 있습니다.

쉽게 말하면 하나의 영화 데이터베이스를 **여러 개의 작은 책장(샤드)으로 분할해 저장**하는 것과 같습니다.
그리고 각 **책장마다 영화 정보가 어떻게 정리되어 있는지 목차(Lucene 인덱스)** 가 따로 마련되어 있어, 사용자가 특정 영화 제목을 검색할 때 **모든 책장을 일일이 뒤지지 않고도 해당 영화가 있는 책장을 빠르게** 찾을 수 있습니다.

여기서 주의할 점은 Lucene 인덱스와 elasticsearch 인덱스는 서로 다른 개념이라는 것입니다.

- **Elasticsearch 인덱스**는 영화 데이터와 같이 문서들을 논리적으로 모아둔 컨테이너입니다.
- **Lucene 인덱스**는 각 샤드가 실제로 데이터를 저장하고 검색을 수행하는 low-level 자료구조로, 한 샤드마다 하나의 Lucene 인덱스가 존재합니다.

검색 작업은 **모든 관련 샤드에서 병렬로 진행**되고 코디네이팅 노드가 각 샤드에서 받은 결과들을 모아 정렬한 후 최종적으로 사용자에게 응답을 제공합니다.

또한, 각 샤드에는 **복제본(Replica)** 이 있어서 한 샤드에 문제가 생기더라도 다른 복제본이 대신 그 역할을 할 수 있습니다.
만일 한 샤드가 처리할 수 있는 초당 처리량(TPS)이 `X`라면 `Y`개의 복제본을 통해 이론적으로 `X * Y`의 TPS을 처리할 수 있는데, 코디네이팅 노드는 **복제본까지 모두 활용**해 검색 요청을 각 샤드(기본 + 복제본)로 분산시켜 노드 당 작업 부하를 줄입니다.

**각 샤드는 Lucene 인덱스와 1:1 대응**됩니다.
즉, Elasticsearch 인덱스 내의 각 샤드는 실제 검색과 인덱싱 작업을 수행하는 하나의 Lucene 인덱스입니다.
Elasticsearch는 이러한 Lucene 인덱스들을 여러 노드에 걸쳐 관리함으로써 높은 가용성과 확장성을 제공합니다.

#### Lucene 인덱스의 구조

Lucene 인덱스는 여러 개의 **세그먼트**라는 작은 단위로 이루어져 있고, 세그먼트는 검색 엔진의 기본 단위입니다.

여기서 "세그먼트"란 문서 데이터를 인덱싱한 후 저장하는 **불변(immutable)한 컨테이너**를 말합니다. 
한 번 생성된 세그먼트는 변경되지 않습니다.

예를 들어 영화 정보가 새로 추가되면 Elasticsearch는 즉시 기존 인덱스를 수정하지 않고,
먼저 문서를 **메모리 버퍼에 임시로 저장한 후 일정량이 모이면 하나의 세그먼트**로 만들어 디스크에 기록합니다.

만약 데이터가 많이 쌓이면 **여러 세그먼트를 하나로 합치는 병합(merge) 작업**을 수행하여,
이 과정에서 삭제된 문서나 업데이트로 인해 soft-delete 되어있던 불필요한 데이터를 정리합니다.

영화 정보를 **수정**할 때도 기존 세그먼트를 직접 수정하지 않고, 해당 문서를 soft-delete 처리한 후 새 문서를 삽입합니다.

이처럼 수정할 때 바로 덮어쓰지 않는 이유는 **데이터가 한 번 정리되어 저장되면 다시 바꾸지 않는 불변(immutable)한 성질** 때문인데, 이 구조는 여러 장점이 있습니다.

1. **쓰기 성능 향상**: 새로운 문서를 추가할 때 기존 세그먼트 수정 없이 빠르게 추가할 수 있습니다.
2. **효율적인 캐싱 가능**: 세그먼트는 변경되지 않으므로 메모리나 SSD에 안전하게 캐시할 수 있습니다.
3. **간소화된 동시성 처리**: 읽기 작업 시 데이터가 중간에 변경되지 않기 때문에 여러 사용자가 동시에 접근해도 문제가 발생하지 않습니다.
4. **쉬운 복구**: 시스템 장애가 발생해도 세그먼트 상태가 일정하므로 복구에 용이합니다.
5. **최적화된 압축**: 불변 데이터는 더 효과적으로 압축할 수 있습니다.
    
    데이터가 변하지 않으므로 압축 알고리즘은 반복되는 패턴이나 중복된 정보를 더 빨리 찾아내고 이를 반영할 수 있습니다.
    그리고 데이터가 변경되지 않으므로 한 번 압축 작업을 최적화해 두면 이후 다시 재계산할 필요가 없습니다.
    
6. **빠른 검색**: 검색을 위한 자료구조와 알고리즘을 최적화할 수 있습니다.

하지만 불변 구조는 **주기적으로 세그먼트를 병합**해야하고, 병합이 진행되기 전까지는 **임시 저장 공간이 증가**한다는 단점도 있습니다.

#### Lucene 세그먼트의 주요 기능

Lucene 세그먼트는 단순히 문서를 저장하는 컨테이너 이상의 역할을 하는데, 세그먼트 내부에는 검색에 최적화된 다양한 자료구조가 들어있습니다.

그 중 가장 중요한 두 가지는 다음과 같습니다.

1. **Inverted Index**

    Inverted Index는 모든 문서에서 등장하는 **고유한 단어**들을 **Key**로 하여 각 단어가 등장하는 **문서들의 ID 목록을 저장**합니다.
        
    예를 들어, 10억 편의 영화 중에서 "인셉션"이라는 단어가 들어간 영화만 찾고 싶을 때, 모든 영화를 처음부터 끝까지 뒤지는 대신 "인셉션"에 해당하는 문서 ID 목록만 빠르게 조회할 수 있습니다.
    이렇게 하면 검색 시간이 O(n)에서 O(1) 정도로 크게 단축됩니다.   
    
2. **Doc Values**

    만약 검색 결과를 가격이나 평점처럼 특정 필드를 중심으로 **정렬**해야 한다면 Doc Values라는 자료구조가 사용됩니다.
    
    Doc Values는 모든 문서의 해당 필드 값을 **column 단위로 연속적으로 저장**하고, 전체 문서를 읽지 않고도 필요한 데이터만 빠르게 읽어올 수 있게 도와줍니다.
    (Spark나 Redshift같은 대규모 데이터 분석 도구들도 유사한 방식으로 데이터를 저장합니다.)

#### 코디네이팅 노드
**코디네이팅 노드**는 검색 요청을 받고 이를 처리할 때 **"어떤 방식으로 검색하는 것이 가장 빠를까?"** 라는 결정을 내리기도 합니다.

여기서 가장 중요한 단계는 **Query planning** 입니다.

쿼리 플래너는 검색 쿼리를 분석하여 어떤 방식으로 검색하는 것이 가장 효율적인지 결정합니다.
예를 들어, "크리스토퍼 놀란" 감독의 영화를 검색할 때, 놀란 감독의 **이름이 영화 데이터 전체에 얼마나 자주 등장**하는지, **영화 제목이나 설명의 길이**는 어떤지 등의 정보를 분석해서 효율적인 검색 순서를 결정합니다.

즉, 쿼리 플래너는 각 필드의 통계, 인기 키워드, 문서 길이 등 여러가지 요소를 고려하여 검색 순서를 선택하고, 여러 노드의 결과를 어떻게 합칠지 결정합니다.

## 고려 사항
1. Elasticsearch는 **검색 전용 엔진**이므로 데이터의 영구 저장은 다른 데이터베이스에 하고, elasticsearch는 그 데이터에 대한 검색을 제공하는 용도로 사용하는 것이 좋습니다.
2. Elasticsearch는 읽기/검색 작업에 최적화되어 있으므로, 만약 **데이터가 자주 수정된다면 성능이 떨어질 수** 있습니다.
3. 검색 결과가 **실시간으로 최신 정보를 반영하지 않을 수** 있습니다. (**eventual consistency**) 그래서 실시간성이 중요한 경우에는 다른 대안을 고려해야 합니다.
4. 데이터를 효율적으로 검색하기 위해서는 **데이터 구조를 미리 잘 설계(비정규화)** 해야 합니다.
5. Elasticsearch와 주 **데이터 저장소간의 데이터 동기화 문제**에 주의해야 합니다.

## 특징
1. 데이터가 **한 번 저장되면 바뀌지 않는 불변성**을 이용하면 데이터의 캐싱, 압축, 동시 접근 등이 훨씬 용이해집니다.
2. **검색 요청을 처리하는 과정과 데이터를 저장하는 과정을 분리**하여 각각 독립적으로 최적화할 수 있습니다.
    
    예를 들어 데이터의 양이 너무 많아 저장과 인덱싱 처리 능력이 중요하다면 데이터 노드를 추가하여 저장 공간과 인덱싱 성능을 확장시킬 수 있고,
    사용자가 검색 요청을 많이 보내는 경우에는 코디네이팅 노드를 추가하여 쿼리 처리 능력을 강화할 수도 있습니다.
    
3. 특정 **검색 패턴에 맞춰 데이터를 저장하는 인덱싱 전략**은 검색 성능에 큰 영향을 미칩니다.
4. 여러 대의 노드가 함께 작업하는 분산 시스템은 확장성과 fault tolerance를 제공하지만, 데이터 일관성이나 네트워크 문제 등 복잡한 문제들도 함께 발생하므로 **CAP Theorem 사이의 균형**을 잘 맞춰야합니다.
5. 특정 용도에 맞춰 **최적화된 자료구조를 선택**하는 것이 성능 향상에 매우 중요합니다.

## References
https://www.paradedb.com/blog/elasticsearch_vs_postgres

https://medium.com/swlh/bkd-trees-used-in-elasticsearch-40e8afd2a1a4

https://j.blaszyk.me/tech-blog/exploring-apache-lucene-index/
4:T6154,
오늘은 이벤트 브로커와 메시지 브로커, 그리고 각각의 대표적인 예시인 Apache Kafka와 RabbitMQ에 대해 알아보겠습니다.

## 이벤트 브로커 (Event Broker)
이벤트 브로커는 **시스템 내에서 일어나는 다양한 사건이나 상태 변화를 다른 컴포넌트에 전달**하는 중개자 역할을 합니다.

예를 들어, 온라인 쇼핑몰에서는 사용자가 상품을 장바구니에 담거나 결제하는 등의 행동이 이벤트로 발생할 수 있습니다.
이러한 이벤트들은 직접적으로 다른 컴포넌트에 전달되는 것이 아니라 이벤트 브로커를 통해 전달되어 **여러 시스템이나 서비스가 동시에 반응**할 수 있도록 할 수 있습니다.

시스템의 각 부분은 이벤트 브로커를 통해 **서로 분리되어 동작**하게 되는데, 이를 통해 한 부분에서 발생한 변화가 다른 부분에 영향을 주더라도 서로 강하게 의존하지 않게 됩니다.

만일 결제 시스템에 문제가 생겨도 주문 처리나 재고 관리 시스템은 이미 발생한 이벤트를 기반으로 자신의 업무를 계속 진행할 수 있습니다.

이런 구조 덕분에 장애가 발생했을 때 **문제를 국소적으로 격리**할 수 있고 **시스템 전체의 안정성을 높일 수 있는 효과**가 있습니다. (Loose coupling)

또한, 이벤트 브로커는 발생한 이벤트 데이터를 기록하고 저장하는 기능도 제공합니다. 
예를 들어 사용자가 웹사이트에서 발생시킨 모든 행동 기록을 이벤트 브로커가 수집해 저장해 놓으면 나중에 이 데이터를 분석해 **사용자 행동 패턴을 파악**하거나 **문제 발생 시 재처리할 수 있는 기반**을 마련할 수 있습니다.

이러한 기능은 **실시간 모니터링**이나 **로그 수집**, **감사**와 같은 다양한 활용 사례에 유용하게 쓰입니다.

### Pub/Sub 모델
이벤트 브로커는 pub/sub 모델을 채택하고 있는데, 이 모델에서 이벤트를 생성하는 쪽은 **publisher**, 이벤트를 받아 처리하는 쪽은 **consumer**라고 부릅니다.

Publisher는 자신이 **생성한 이벤트를 이벤트 브로커에 전달**하고, 이벤트 브로커는 이 **이벤트를 구독하고 있는 여러 consumer들에게 동시에 전달**합니다.

예를 들어, 온라인 쇼핑몰에서 상품이 구매되면 결제 시스템, 주문 관리 시스템, 재고 관리 시스템, 심지어는 고객에게 알림을 보내는 시스템 등이 모두 이 이벤트를 받아 각각의 역할을 수행할 수 있습니다.

다른 예시를 생각해보면 이벤트 브로커는 마치 **도서관의 사서**와 같다고 볼 수 있습니다.

도서관에서 책을 요청하면 사서가 책을 찾아 여러 독자에게 전달해 주는 것처럼, 이벤트 브로커도 각 시스템의 요청이나 상태 변화를 받아 필요한 모든 곳에 전달합니다.
이 과정에서 각 시스템은 직접 서로를 호출할 필요 없이 **간접적으로 소통**할 수 있게 됩니다.

### Apache Kafka
Kafka는 이벤트 브로커의 대표적인 예시로 주로 **대용량의 실시간 데이터 스트림**을 처리하는 데 최적화되어 있습니다.

Kafka에서는 **모든 이벤트가 로그 형태로 기록**되며, 이 로그는 분산 시스템 내 여러 노드에 걸쳐 저장됩니다.

예를 들어 대규모 웹 애플리케이션에서 사용자 행동 데이터나 서버 로그 같은 이벤트가 발생하면
이 데이터를 Kafka에 기록해 여러 consumer가 동시에 실시간 분석, 모니터링, 경고 등의 작업을 수행할 수 있습니다.

Event broker의 특성처럼 Kafka의 pub/sub 모델은 publisher가 데이터를 보내면 여러 소비자가 그 로그를 구독하여 각자의 필요에 따라 데이터를 처리할 수 있습니다.
이 과정에서 이벤트가 지속적으로 기록되기 때문에 **나중에 필요할 때 과거의 데이터를 재분석하거나 시스템 에러 발생 시 재처리**를 쉽게 할 수 있습니다.

#### 예제
월드컵처럼 실시간 이벤트가 많은 대회를 다루는 웹사이트를 운영한다고 가정해봅시다.

경기가 진행되는 동안 골이 들어가거나 선수 교체가 이루어질 때마다 해당 정보를 빠르게 업데이트해야 합니다.

이를 위해 이벤트가 발생할 때마다 **큐(queue)** 에 저장하며, 이벤트를 큐에 삽입하는 서버나 프로세스를 **producer**라고 부릅니다.

반대로, 큐에서 이벤트를 읽고 웹사이트를 업데이트하는 서버를 **consumer**라고 합니다.

현재 월드컵은 48개 팀이 참가하지만, 만약 1000개 팀이 동시에 경기를 진행하는 대회로 확장된다면 발생하는 이벤트 수가 급증할 것입니다.

이 경우, 하나의 서버가 큐를 관리하는 방식으로는 부담을 감당하기 어렵고, consumer 또한 넘쳐나는 데이터를 처리하지 못할 것입니다.
이 때는 **시스템을 확장하여 여러 대의 서버를 활용**해야 하는 상황이 발생합니다.

그러나 단순히 여러 서버에 이벤트를 무작위로 분배하면 문제가 생길 수 있습니다.
예를 들어, 어떤 서버에서는 경기가 시작되기 전에 골이 들어가는 것으로 표기될 수도 있고, 선수가 경고를 받기 전에 이미 퇴장당한 것으로 기록될 수도 있습니다.

따라서 **이벤트 순서를 유지**하면서도 여러 서버에서 부하를 나누어 처리하는 방법이 필요합니다.

Kafka의 핵심 개념 중 하나는 **사용자가 직접 메시지의 분배 전략을 정의할 수 있다**는 것입니다.

위 예제에서 가장 적절한 방법은 **경기단위로 이벤트를 분배하는 것**입니다. 즉, **같은 경기에 대한 이벤트는 동일한 큐(Partition)에 저장되도록 구성**하면 한 경기 내에서는 모든 이벤트가 순서대로 처리될 수 있습니다.

하지만 consumer가 **처리해야 할 데이터가 너무 많다**면 어떻게 해결할 수 있을까요?

Kafka에서는 **consumer group**을 활용해 이 문제를 해결할 수 있습니다. 여러 consumer가 같은 consumer group에 속하면 Kafka는 **각 이벤트가 오직 하나의 consumer에게만 할당되도록 보장**합니다.
그럼 부하를 여러 consumer가 나누어 처리하면서도 중복 처리를 방지할 수 있습니다.

만약 웹사이트가 축구뿐만 아니라 농구 같은 다른 스포츠 이벤트도 다루기로 결정했다면 Kafka의 **topic** 개념을 활용할 수 있습니다.

**각 이벤트는 특정한 topic에 속하며, consumer는 특정한 토픽만 구독**하면 됩니다. 예를 들어 축구 웹사이트 전용 consumer는 축구 토픽만 구독하고, 농구 웹사이트 전용 consumer는 농구 토픽만 구독하면 됩니다.

#### 기본 개념
앞서 설명한 예제를 기반으로 Kafka의 핵심 개념에 대해 좀 더 알아보겠습니다.

1. **Kafka 클러스터와 브로커**
    
    Kafka 클러스터는 여러 개의 **브로커**로 구성 됩니다.
    브로커는 각각 독립적인 서버로, 데이터를 저장하고 클라이언트 요청을 처리하는 역할을 합니다.
    브로커가 많을수록 더 많은 데이터를 저장할 수 있고, 더 많은 클라이언트를 처리할 수 있습니다.

2. **파티션과 로그 구조**

    각 브로커는 여러 개의 **파티션**을 가집니다.
    파티션은 **불변(immutable)한 메시지의 순차적인 저장 공간**으로, 새로운 메시지가 계속 추가되는 구조입니다.
    이 구조는 **로그(log) 파일**과 유사하게 생각하면 됩니다.
    Kafka는 파티션 단위로 데이터를 분산 저장하고 병렬로 처리하기 때문에 파티션을 통해 시스템을 확장할 수 있습니다.

3. **토픽과 파티션의 차이**
    
    **토픽은 파티션을 논리적으로 그룹화하는 개념**입니다.
    Kafka에서 데이터를 주고받을 때는 항상 특정 토픽을 통해 이루어집니다.
    토픽은 항상 **다중 producer를 허용**하며, 하나의 토픽에는 0개, 1개, 또는 여러 개의 producer가 데이터를 쓸 수 있습니다.

4. **Kafka의 메시지 큐 vs. 스트림 처리 방식**
    
    Kafka는 **메시지 큐(Message Queue)** 로 사용할 수도 있고, **스트림(Stream)** 으로 사용할 수도 있습니다.
    
    메시지 큐 방식은 consumer가 메시지를 읽은 후 **처리가 완료되었음을 명시적으로 확인**합니다.
    스트림 방식은 consumer가 메시지를 읽고 처리는 하지만 **Kafka에 처리 완료를 하지는 않습니다**.

#### 동작 원리
Kafka에서 이벤트가 발생하면 producer는 메시지를 포맷팅한 후 토픽으로 전송합니다.
Kafka의 메시지는 필수 필드인 **value**와 선택 필드인 **key, timestamp, headers**로 구성됩니다. 

    Key: 메시지가 어느 파티션에 저장될지를 결정하는데 사용됩니다. key가 없으면 Kafka는 메시지를 무작위로 파티션에 배치합니다.
    Timestamp: 메시지의 순서를 결정하는데 사용됩니다.
    headers: HTTP header처럼 키-값 쌍으로 메타데이터를 저장할 수 있습니다. 

1. **파티션 할당 및 브로커 처리**
    
    Kafka는 메시지의 키를 해싱하여 특정 파티션에 할당합니다.
    키가 없는 경우 **라운드 로빈**이나 설정된 다른 로직을 이용해 파티션을 배정합니다.
    여기서 **같은 키를 가진 메시지는 항상 같은 파티션에 저장되어 순서가 유지**됩니다.
    
    Kafka는 메시지가 할당된 **파티션을 어느 브로커가 관리하는지 확인**하기도 합니다.
    **Kafka 컨트롤러**가 이 메타데이터를 유지하고 Producer는 해당 브로커로 메시지를 전송하게 됩니다.

2. **Kafka의 로그 구조 및 메시지 처리 방식**

    **파티션**은 **append-only log** 형태로 동작하는데, 메시지는 끝에 추가되며 수정이나 삭제되지 않습니다.
    
    이 방식이 가지는 장점은 다음과 같습니다.
        
        불변성 (Immutability): 메시지가 한 번 저장되면 변경되지 않아 일관성이 유지됩니다.
        고성능 (Efficiency): 디스크의 순차 쓰기(sequential write)를 활용하여 높은 처리량을 제공합니다.
        확장성 (Scalability): 파티션을 여러 브로커에 분산시켜 시스템 부하를 효율적으로 분배할 수 있습니다.

    Kafka의 메시지는 **각 파티션 내에서만 순서가 보장**되며, 각 메시지는 **offset**을 부여받습니다.
    여기서 offset은 **특정 파티션에서의 메시지 위치**를 나타내고, consumer가 메시지를 읽을 때 이걸 기준으로 진행 상태를 관리합니다.
    
3. **복제 및 내구성**

    Kafka는 **leader-follower** 모델을 이용해 데이터 복제를 수행합니다.
        
        리더 복제본 (Leader replica): 각 파티션의 리더는 메시지의 읽기 및 쓰기를 담당합니다.
        팔로워 복제본 (Follower replica): 다른 브로커에 분산 저장되어 리더 복제본을 백업하는 역할을 합니다.
        동기화 및 장애 복구 (sync and failover): 팔로워들은 리더의 메시지를 동기화하며 최신 데이터를 유지합니다.
            만약 리더에 장애(failure)가 발생하면 최신 데이터를 가진 팔로워가 새로운 리더로 승격(promote)됩니다.
            Kafka의 컨트롤러가 전체 복제 및 장애 조치를 관리합니다.
    
4. **Consumer의 동작 방식**
    
    consumer는 토픽에서 메시지를 읽어오는 역할을 하는데 두 가지 방식으로 동작할 수 있습니다.
    
    1. **Push model**: 메시지가 도착하면 즉시 consumer에게 전달
    2. **Pull model**: consumer가 일정 주기로 Kafka에서 메시지를 조회    

#### Kafka는 언제 사용해야 할까?
위에서 정리했다시피 Kafka는 **메시지 큐** 또는 **스트림**으로 활용될 수 있으며 두 방식의 가장 큰 차이는 **consumer가 데이터를 처리하는 방식**에 있습니다.

1. **메시지 큐로 사용하는 것이 적절한 경우**
    
    1. **비동기 처리가 필요한 경우**:
    Youtube와 같은 비디오 플랫폼을 예로 들어봅시다. 사용자가 동영상을 업로드하면 저화질 버전은 즉시 제공하고 고화질 인코딩은 Kafka 토픽에 메시지를 추가하여 여유가 생길 때 처리할 수 있습니다.
    
    2. **메시지의 순서를 보장해야 하는 경우**:
    온라인 티켓 예매 시스템 같은 경우 사용자가 도착한 순서대로 티켓 구매 페이지에 접근해야 합니다.
    Kafka를 **가상 대기열로 활용**하면서 사용자가 접근한 **순서를 유지하며 메시지를 소비하도록 보장**할 수 있습니다.
    
    3. **Producer와 consumer를 분리하여 독립적으로 확장해야 하는 경우**:
    일반적으로 **producer가 메시지를 생성하는 속도가 consumer가 처리하는 속도보다 빠를 때** 발생합니다. 
    MSA (Microservice Architecture) 에서는 서비스 간 결합도를 낮추고, 특정 서비스가 다운되더라도 다른 서비스에 영향을 주지 않도록 비동기 메시지 큐를 활용합니다.
    
2. **스트림으로 사용하는 것이 적절한 경우**
    
    1. **실시간으로 데이터를 지속적으로 처리해야 하는 경우**:
    광고 클릭 데이터를 실시간으로 수집/집계하는 Ad Click Aggregator 시스템을 예로 들 수 있습니다.
    Kafka를 활용하면 **실시간으로 수집된 클릭 데이터를 빠르게 분석하고 광고주에게 즉시 피드백을 제공**할 수 있습니다.
    
    2. **여러 consumer가 동시에 메시지를 처리해야 하는 경우**:
    Youtube live 댓글 시스템 같은 실시간 방송 플랫폼에서는 댓글을 여러 consumer에게 동시에 전달해야 합니다.
    Kafka를 pub/sub system으로 활용하면 **여러 consumer가 동일한 메시지를 소비할 수 있도록 보장**할 수 있습니다.

#### 단일 브로커의 성능 한계
Kafka를 이용한 설계를 할 때는 먼저 단일 Kafka 브로커의 한계를 이해하는 것이 중요합니다.
**예상되는 메시지 처리량(throughput)** 과 **메시지 크기**를 잘 고려하여 확장이 필요한지에 대한 여부를 판단해야 합니다.

Kafka 메시지 크기에는 **hard limit이 없지만** 설정 파일에서 `message.max.bytes`를 통해 조정할 수 있습니다.
그러나 최적의 성능을 위해선 **메시지 크기를 1MB 이하로 유지**하는 것이 권장됩니다.
메시지가 작을수록 메모리 부담이 줄어들고 네트워크 활용이 최적화되기 때문입니다.

또한, Kafka는 데이터베이스가 아니며 대형 파일 저장용 시스템도 아니라는 점을 명심해야 합니다.
**메시지는 빠르게 처리될 수 있어야 하고 큰 데이터를 직접 저장하는 것은 비효율적**입니다.

예를 들어 Youtube같은 동영상 플랫폼을 설계할 때 업로드된 동영상을 Kafka에 저장하는 것이 아니라 S3 같은 분산 파일 시스템에 저장하고, Kafka 메시지에는 **해당 파일의 위치만 저장**하는 것이 올바른 접근법입니다.

좋은 하드웨어 환경에서는 단일 브로커가 약 **1TB의 데이터를 저장**하고 **최대 100만개의 메시지를 초당 처리**할 수 있습니다.
다만, 이는 메시지 크기와 하드웨어 사양에 따라 다를 수 있고 일반적인 개략적 추정치입니다.
만약 Kafka의 처리량이 이 범위를 넘지 않는다면 확장을 고려할 필요가 없을 수도 있습니다.
    
#### 확장 전략
1. **수평 확장 (Horizontal scaling)**
    
    가장 단순한 방법은 **Kafka 클러스터에 더 많은 브로커를 추가**하는 것입니다.
    브로커를 추가하면 로드가 분산되며 장애 내성이 향상됩니다.
    
    그러나 브로커를 추가하는 것만으로는 확장이 되지 않는데, **토픽의 파티션 개수를 충분히 설정**해야 브로커 추가의 효과를 볼 수 있습니다.
    파티션이 충분하지 않으면 새로 추가된 브로커가 활용되지 않으므로 파티션 개수 증가가 필수입니다.
    
2. **파티셔닝 전략 (Partitioning strategy)**
    
    가장 중요한 확장 전략은 **어떻게 데이터를 파티션할 것인지 결정하는 것**입니다.
    
    Kafka는 메시지 키의 해시값을 기반으로 파티션을 결정하는데 잘못된 키 선택은 특정 파티션에 과부하(**Hot partition**)를 발생시킬 수 있습니다. 
    좋은 키를 선택하려면 트래픽이 고르게 분산될 수 있도록 설계해야 합니다.

#### Hot Partition 문제 해결 전략
1. **무작위 파티셔닝 (Random Partitioning)**
    
    키를 제공하지 않으면 Kafka는 메시지를 랜덤한 파티션으로 분배합니다.
    이 방법은 트래픽이 균등하게 분산된다는 장점이 있지만 **메시지 순서 보장이 어렵습니다.**
    메시지 순서가 중요하지 않은 경우에 사용 가능합니다.
    
2. **랜덤 솔팅 (Random Salting)**
    
    **키 값에 랜덤 값(숫자, Timestamp 등)을 추가**하여 분산을 유도하는 기법입니다.
    예를 들어, 광고 클릭 로그를 `ad_id` 기반으로 파티셔닝하면 특정 인기 광고에 트래픽이 집중될 가능성이 있습니다.
    
    여기서 `ad_id + random_salt`를 사용해서 여러 파티션에 트래픽을 분산할 수 있습니다.
    단, **consumer에서 데이터를 집계하는 로직이 복잡**해질 수 있습니다.
    
3. **복합 키 (Compound Key) 사용**
  
    단일 `ad_id` 대신 `ad_id + geolocation`, `ad_id + user_id` 등 복합 키를 사용하여 트래픽을 분산할 수 있습니다.
   
4. **백 프레셔 (Back Pressure) 적용**
    
    과부하가 발생하면 **producer가 메시지를 생성하는 속도를 늦추도록** 조정할 수도 있습니다.
    
#### 성능 최적화

Kafka를 이벤트 스트림으로 사용할 경우 성능 최적화가 중요해집니다.

이 때, **배치 전송**과 **메시지 압축** 등과 같은 기법을 사용할 수 있습니다.

배치 전송은 producer가 메시지를 개별적으로 전송하는 대신 일정량을 모아서 한 번에 전송하는 방법입니다.

또한, KafKa는 GZIP, Snappy, LZ4 등의 압축 알고리즘을 지원하며 압축을 활성화하면 네트워크 전송 속도를 높이고 저장 공간을 절약할 수 있습니다.

## 메시지 브로커 (Message Broker)
메시지 브로커의 주요 목적은 **메시지를 안정적으로 전송하고 각 시스템이 독립적으로 작동**할 수 있도록 하는 것입니다.

메시지 브로커는 메시지를 **Queue나 Topic과 같은 구조에 저장**하고, consumer가 준비되면 해당 메시지를 전달하여 순차적 또는 병렬적으로 처리할 수 있게 합니다.

예를 들어, 사용자가 주문을 완료하면 메시지 브로커는 이 주문 데이터를 결제 처리 시스템에 전달하고, 결제가 완료된 후 다시 주문 처리 시스템에 결과를 전달하는 과정을 중개합니다.

이벤트 브로커가 도서관의 사서와 같았다면 메시지 브로커는 **우편 배달 시스템**과 비슷하다고 볼 수 있습니다.

편지를 보내는 사람이 직접 수취인에게 전달하는 대신 우체국에 맡기는 것처럼 메시지 브로커는 발신자로부터 메시지를 받아 중간에서 필요한 곳으로 전달합니다.

### RabbitMQ
RabbitMQ는 전통적인 메시지 브로커의 대표적인 예입니다.

RabbitMQ에서는 **메시지가 큐에 저장되고, consumer가 해당 큐에서 하나씩 메시지를 가져와 처리하는 방식**으로 동작합니다.

예를 들어 전자상거래 시스템에서 주문이 들어오면 주문 처리 요청 메시지가 RabbitMQ 큐에 저장되고, 주문 처리 서비스가 이 큐에서 메시지를 하나씩 꺼내 처리합니다.

RabbitMQ의 **라우팅 기능**을 이용하면 특정 조건이나 주제에 따라 메시지를 다양한 큐로 분배할 수 있어 시스템 간의 통신을 보다 세밀하게 조절할 수 있습니다.

#### 동작 원리
RabbitMQ의 기본 구성 요소는 크게 **producer, consumer, exchange, queue, binding**으로 구분할 수 있습니다.

**Producer**는 애플리케이션에서 **메시지를 생성하고 RabbitMQ에 전달**하는 역할을 합니다.
이때 메시지를 직접 queue에 넣는 것이 아니라, 먼저 **exchange로 메시지를 전송**합니다.

**Exchang**e는 들어오는 **메시지를 어떻게 queue로 라우팅할 것인지 결정**하는 중추적인 역할을 합니다.
Exchange엔 여러 종류가 있는데, 대표적으로 **direct, topic, fanout, headers** exchange가 있습니다.
각 exchange는 routing key나 바인딩 조건에 따라 메시지를 적절한 queue로 분배합니다.

**Queue**는 **실제로 메시지가 저장되는 공간**입니다.
메시지는 저장된 후, 준비된 consumer에게 전달되어 처리됩니다.

RabbitMQ는 메세지의 신뢰성과 안정성 보장을 위해 큐에 저장된 메시지에 대해 **persistence 옵션을 제공**하며, 
consumer가 메시지를 받아 처리한 후에는 메시지에 대한 **ACK을 받아야 메시지를 삭제**합니다.
이 과정은 메시지 유실이나 중복 처리 방지에 중요한 역할을 합니다.

**Binding**은 exchange와 queue 사이의 **연결 규칙을 정의**합니다.
Binding을 통해 특정 routing key나 패턴에 맞는 메시지가 어떤 queue로 전달될지 결정되므로, 메시지 흐름을 세밀하게 정의할 수 있습니다.

**Consumer**는 큐에 저장된 **메시지를 받아 처리**하는 역할을 합니다.
여러 consumer가 하나의 큐를 구독할 경우, RabbitMQ는 메시지를 **라운드 로빈 방식** 등으로 분배하여 각 consumer가 메시지를 균등하게 처리할 수 있도록 지원합니다.

Consumer가 메시지를 처리하고 난 후 ACK을 보내야하는데, 만약 메시지 처리에 실패하면 RabbitMQ는 해당 메시지를 **재전달하거나 DLX 또는 다른 큐로 라우팅** 할 수 있습니다.


## 차이점
이벤트 브로커는 **"어떤 일이 발생했다"는 사실 자체를 전파**하는 데 중점을 둡니다. 예를 들어 온라인 쇼핑몰에서 사용자가 결제를 완료했을 때 그 사건을 기록하고 여러 시스템에 동시에 알리는 역할을 할 수 있습니다.

반면, 메시지 브로커는 **특정 작업이나 요청을 안전하게 전달**하는 데 집중합니다.
예를 들어, 주문 처리를 위해 결제 요청을 보내거나 작업 큐에 저장된 작업을 하나씩 처리하는 시스템에서 메시지 브로커는 메시지를 큐잉하여 전달하고, 재시도나 배달 보증 등의 기능을 통해 데이터의 신뢰성을 보장합니다.

또한 이벤트 브로커는 이벤트 자체를 **지속적으로 기록하고 저장**하여 나중에 재처리나 분석에 활용할 수 있는 반면, 메시지 브로커는 일반적으로 메시지를 즉시 처리하는 **단기적인 데이터 전달**에 집중합니다.

이처럼 이벤트 브로커는 주로 **시스템의 상태 변화나 사건 발생의 "기록"** 을 다루고, 메시지 브로커는 **특정 작업의 "수행"** 과 관련된 데이터를 다루는 데 초점을 맞춥니다.

Kafka와 RabbitMQ 사용 사례를 비교하자면 Kafka는 **높은 처리량, 내구성, 재처리 기능** 등이 중요한 이벤트 스트리밍 환경에, RabbitMQ는 **복잡한 라우팅, 메세지 우선 순위, 큐 기반의 작업 분산 처리**와 같은 요구 사항이 있을 때 효과적인 선택이 됩니다.

## References
https://medium.com/riskified-technology/message-broker-vs-event-broker-when-to-use-each-one-of-them-15597320a8ba#:~:text=I%20would%20like%20to%20discuss,have%20a%20working%20experience%20with.
https://kafka.apache.org/uses
https://www.cloudamqp.com/blog/rabbitmq-use-cases-explaining-message-queues-and-when-to-use-them.html
https://www.youtube.com/watch?v=DU8o-OTeoCc&ab_channel=HelloInterview-SWEInterviewPreparation
https://medium.com/riskified-technology/message-broker-vs-event-broker-when-to-use-each-one-of-them-15597320a8ba
5:T5592,
`Togather` 프로젝트 (북미 대학생을 위한 익명 커뮤니티 앱) 를 만들면서 자연스럽게 커뮤니티 플랫폼과 채팅 서비스의 시스템 디자인에 대해 관심을 갖게 되었습니다.

이번 글에서는 대규모 사용자를 대상으로 한 채팅 서비스의 시스템 디자인에 대해 살펴보겠습니다.

## 기능적 요구사항
1. **그룹 채팅 지원** – 여러 명이 함께 대화할 수 있는 그룹 메시지 기능이 필요함.
2. **메시지 송수신 기능** – 사용자가 메시지를 보내고 받을 수 있어야 함.
3. **오프라인 수신 가능** – 사용자가 오프라인 상태일 때도 메시지를 받을 수 있어야 하며, 다시 온라인이 되면 확인할 수 있어야 함.
4. **사진 및 미디어 전송 지원** – 텍스트뿐만 아니라 사진, 동영상 등 미디어 파일도 주고받을 수 있어야 함.

## 비기능적 요구사항
1. **빠른 메시지 전달 속도** – 온라인 상태인 사용자는 500ms(0.5초) 이내에 메시지를 받아야 함.
2. **메시지 전달 보장** – 메시지가 유실되지 않고 반드시 수신자에게 전달되어야 함.
3. **확장성** – 수십억 명의 사용자가 이용해도 원활하게 동작해야 함.
4. **필요한 메시지만 저장** – 메시지는 필요한 만큼만 보관하고, 불필요한 데이터는 자동으로 삭제되도록 관리해야 함.
5. **안정성 보장** – 특정 서버나 기능이 고장 나더라도 전체 서비스가 중단되지 않도록 시스템이 복구 및 대응할 수 있어야 함.
## Core entities
- Users
- Chats
- Messages
- Clients (devices)
## API 디자인
메시지가 매우 자주 주고받아지는 환경에서는 매번 요청을 보내고 응답을 받는 REST API 방식은 비효율적입니다. REST API는 요청이 올 때마다 새로운 연결을 만들고, 응답을 받은 후 연결을 종료하기 때문에 실시간성이 중요한 서비스에서는 지연이 발생할 수 있습니다.

반면, 양방향 소켓 연결 (bi-directional socket connection) 은 한 번 연결을 설정하면 계속 유지되므로 서버와 클라이언트가 실시간으로 데이터를 주고받을 수 있습니다. 
예를 들어, 채팅 서비스에서는 사용자가 메시지를 보내면 서버가 즉시 상대방에게 전달해야 하는데 소켓 연결을 사용하면 별도의 요청 없이도 빠르게 메시지를 받을 수 있습니다.


즉, 자주 변하는 데이터를 실시간으로 주고받아야 하는 서비스에서는 REST API보다 소켓 연결이 훨씬 효율적입니다.

### 전송되는 명령
#### Create Chat: 대화방 생성
**Request**
```json
{
    "participants": ["user1", "user2"],
    "name": "Study Group"
}
```

#### Send Message: 메시지 전송
**Request**
```json
{
    "chatId": 1,
    "message": "hi",
    "attachments": ["sampleFile"]
}
```

#### Create Attachment: 첨부파일 생성
**Request**
```json
{
    "body": ...,
    "hash": ""
}
```

#### Modify Chat Participants, 대화방 인원 수정
**Request**
```json
{
    "chatId": 1,
    "userId": 1,
    "operation": "ADD" | "REMOVE"
}
```
위에 나열한 각 요청은 다른 클라이언트들에게도 동시에 전송됩니다. 클라이언트가 요청을 받으면 서버에 `"명령을 정상적으로 받았다"`는 ACK (확인 응답) 메시지를 보냅니다.

이렇게 하면 서버는 `"이제 이 메시지를 다시 보낼 필요가 없겠구나"` 하고 확인합니다. (만약 ACK을 받지 못하면 서버는 메시지가 제대로 전달되지 않았다고 판단하고 다시 보낼 수도 있습니다.)


### 수신되는 명령
#### New Message, 새로운 메시지 수신
**Request**
```json
{
    "chatId": 1,
    "userId": 1
    "message": "hi",
    "attachments": []
}
```

#### Chat Update, 대화방 업데이트
**Request**
```json
{
    "chatId": 1,
    "participants": ["user1"],
}
```

## High-Level Design
### 1. 그룹 채팅 지원: 최대 100명
```plaintext
+-----------+   (WebSocket Conn)    +---------+      +-------------+      +--------------------+
|  Client   | --------------------> |  L4 LB  | ---> | Chat Server | ---> | Database (DynamoDB) |
+-----------+                       +---------+      +-------------+      +--------------------+
```

#### 플로우
1. 사용자가 서비스에 연결한 후 createChat 요청을 보냅니다.
2. 서버는 한 트랜잭션 내에서 새로운 채팅방(Chat) 데이터를 생성하고, 해당 채팅방의 참여자(ChatParticipant) 정보도 함께 저장합니다.
3. 채팅방이 성공적으로 생성되면 서버는 생성된 chatId를 사용자에게 반환합니다.

#### 사용 기술
- **L4(4계층) 로드밸런서**: 웹소켓 연결을 지원하며 실시간 통신이 필요한 메시징 서비스에 적합합니다.
- **AWS DynamoDB**: 채팅방 생성 시 관련 데이터(참여자 정보, 생성 시간 등)를 저장하기 위해 사용됩니다. 빠른 Key-Value 성능과 뛰어난 확장성(Scalability) 을 제공하여 대규모 사용자 환경에서도 안정적으로 동작합니다.

여기서 다른 데이터베이스가 아닌 `AWS DynamoDB`를 사용하는 이유는 다음과 같습니다.
- **확장성**: 자동으로 수평 확장 되므로 사용자가 많이 늘어나는 상황에서도 안정적으로 동작합니다. RDBMS는 일정 규모가 넘어가면 샤딩을 직접 관리해야 하는데, DynamoDB는 이를 자동으로 처리해줍니다. 같은 NoSQL 데이터베이스인 MongoDB와 같은 경우에도 수평 확장이 가능하지만, 샤딩과 클러스터 관리를 직접 해야합니다.
- **Low latency**: key-value 기반이라 초당 수백만건의 요청을 빠르게 처리할 수 있습니다. 지연 시간을 최소화 하기 좋습니다. 쿼리 기능은 제한적이지만 Composite key와 GSI를 이용해 특정 조회 패턴을 빠르게 지원합니다.
- **비용 효율성**: 온디맨드 모델을 사용하면 실제 사용한 만큼만 비용을 지불합니다. 고성능 환경에서 RDBMS를 유지하려면 서버 증설 등에 비용이 크게 늘어날 수 있습니다.

#### DB Index 설계: `ChatParticipant` 테이블
`ChatParticipant` 테이블은 다음과 같은 두 가지 기능을 지원해야 합니다.
1. 특정 채팅방에 참여한 모든 사용자 조회
2. 특정 사용자가 참여 중인 모든 채팅방 조회

이를 위해, DynamoDB의 `Composite primary key`와 `GSI(Global Secondary Indexes)`를 활용해야 합니다.
DynamoDB는 테이블을 만들 때 기본 키로 설정되는 두 가지 유형의 Primary key를 지원하는데, 단일 partition key (우리가 아는 기본 primary key), composite key (partition key + sort key) 로 나누어져 있습니다.

DynamoDB의 composite key는 partition key + sort key 조합으로 테이블을 구성하는 방식인데, 같은 Partition key 값을 가진 여러 개의 데이터를 저장할 수 있습니다. Partition key로 데이터를 그룹화하고 Sort key로 정렬하는 방식입니다.


우리가 `chatId`를 **Partition Key**, `participantId`를 **Sort Key**로 설정하면 특정 채팅방(`chatId`)에 속한 모든 사용자를 손쉽게 조회할 수 있습니다. 하지만 **"특정 사용자가 속한 모든 채팅방을 알고 싶다"** 라는 쿼리를 실행하려면 `participantId`를 기준으로 검색해야 합니다. 이를 가능하게 하기 위해 **GSI(Global Secondary Index)** 를 추가해야 합니다.
**GSI**는 DynamoDB에서 테이블 생성 후 추가 가능한 추가적인 조회 패턴을 지원하기 위해 사용되는 인덱스입니다. 여기서 Partition key를 `participantId`로, sort key를 `chatId`로 설정하면 특정 유저가 참여한 모든 채팅방을 효율적으로 조회할 수 있습니다.

GSI가 **"Global"한 이유**는 **기본 테이블의 Partition Key와 상관없이 전역적으로 데이터를 검색할 수 있기 때문**입니다. 반면, **LSI(Local Secondary Index)** 는 특정 Partition 내부에서만 작동하므로 예를 들어 특정 채팅방 내에서 가장 최근 메시지를 검색할 때(`chatId -> timestamp`) LSI를 활용할 수 있습니다.

**요약**
1. Composite Primary Key(`chatId` + `participantId`)를 사용하면 특정 `chatId`에 속한 모든 사용자를 빠르게 조회할 수 있습니다.
2. GSI(`participantId` + `chatId`)를 추가하면 특정 사용자가 속한 모든 채팅방을 효율적으로 검색할 수 있습니다.

### 2. 메시지 송수신 기능
우선 문제를 단순화하기 위해 **서버가 하나만 존재한다고 가정**해보겠습니다. 또한, 앞서 언급한 것처럼 **웹소켓(WebSocket) 연결을 사용하여 실시간 메시지를 주고받도록 설계**합니다.


유저가 채팅 서버에 웹소켓을 통해 연결하면 **서버는 해당 유저의 연결 정보를 해시맵(HashMap)에 저장**합니다. 이렇게 하면 **현재 어떤 유저가 서버에 연결되어 있는지 파악할 수 있으며, 연결된 유저에게 메시지를 직접 전달**할 수 있습니다.

#### 메시지 송신 플로우 (1차 버전)
1. 유저가 `sendMessage` 요청을 서버에 보냅니다.  
2. 서버는 데이터베이스의 `ChatParticipant` 테이블을 조회하여 **해당 채팅방에 속한 모든 참여자 목록을 가져옵니다.**  
3. 서버는 **내부 해시맵을 확인하여 현재 웹소켓 연결이 활성화된 유저들에게만 메시지를 전송합니다.**  

이 방식에서는 다음과 같은 **제약 사항**이 존재합니다.  
- 모든 유저가 웹소켓 연결 상태여야만 메시지를 받을 수 있음
- 유저가 반드시 같은 서버에 연결되어 있어야 함
- 각 유저마다 웹소켓을 유지하고 관리해야 함

위에서 언급한 제약 사항들은 이후 섹션에서 해결 방법을 다룰 예정입니다.  
### 3. 오프라인 수신 기능 (최대 30일)
오프라인 수신 기능을 만들기 위해 앞에서 가정했던 일부 조건들을 다시 생각해보겠습니다. 오프라인 상태인 유저에게 메시지를 전달하려면 메시지를 데이터베이스에 저장해야 할 필요가 생깁니다.

각 유저별로 **메시지 수신함**을 만들고 여기에 **아직 전달되지 않은 메시지들을 저장**하는 방식으로 설계해보겠습니다.
메시지가 전송되면 **수신자의 수신함에 메시지를 저장**하고, 만약 수신자가 온라인 상태라면 메시지 즉시 전달을 시도합니다. 만일 유저가 오프라인 상태라면 메시지를 저장한 후 나중에 다시 접속했을 때 전달하도록 하겠습니다.
#### 메시지 송신 플로우 (2차 버전)
1. 유저가 `sendMessage` 요청을 서버에 보냅니다.  
2. 서버는 데이터베이스의 `ChatParticipant` 테이블을 조회하여 **해당 채팅방에 속한 모든 참여자 목록을 가져옵니다.**  
3. 서버는 한 트랜잭션 내에서 (1) `Message` 테이블에 메시지를 저장하고, (2) 채팅방의 각 참여자의 `Inbox`에 해당 메시지 정보를 저장합니다.
4. 서버는 클라이언트에게 **성공/실패 응답 + `messageId`** 를 반환합니다.  
5. 서버는 **웹소켓 연결 정보 해시맵**을 확인하여 현재 온라인 상태인 수신자에게 `newMessage` 명령을 통해 메시지를 즉시 전달합니다.  
6. **연결된 수신자**는 메시지를 받은 후, **`ack` 메시지를 서버에 반환**합니다. 서버는 `Inbox` 테이블에서 해당 유저에게 전달된 메시지를 삭제하여 중복 전송을 방지합니다.

#### 연결되지 않은 수신자 플로우
오프라인 상태였던 유저가 다시 서버에 연결되었을 때, 이전까지 전달되지 않았던 메시지를 정상적으로 받을 수 있도록 처리해야 합니다.
1. 수신자가 서버에 연결되면 **서버는 해당 유저의 `Inbox` 테이블을 조회하여 아직 남아있는 메시지 ID 목록을 가져옵니다.**  
2. 각 `messageId` 에 해당하는 메시지를 `Message` 테이블에서 조회합니다.  
3. 온라인 상태인 수신자에게 `newMessage` 명령을 통해 메시지들을 전달합니다.
4. **연결된 수신자**는 메시지를 받은 후, **`ack` 메시지를 서버에 반환**합니다. 서버는 `Inbox` 테이블에서 해당 유저에게 전달된 메시지를 삭제하여 중복 전송을 방지합니다.

마지막으로, **간단한 Cron Job을 활용하여 30일 이상 전달되지 않은 `Inbox` 메시지를 정리(cleanup)** 할 수 있습니다. 
### 4. 사진 및 미디어 전송 기능
이상적인 접근 방식은 **사용자가 직접 Blob Storage(예: AWS S3, GCS)에 업로드할 수 있도록 권한을 부여하는 것**입니다.  
이를 위해 **Pre-Signed URL**을 활용하면 채팅 서버를 거치지 않고도 사용자가 직접 파일을 업로드할 수 있습니다. 이 방식은 제가 `Togather` 프로젝트에서 사용자가 게시글을 올릴 때 미디어를 첨부하는 과정에서도 적용했던 방식입니다.

#### 파일 업로드 플로우
1. 사용자가 `getAttachmentTarget` 요청을 **Chat Server**에 보냅니다.  
2. **Chat Server**는 **Pre-Signed URL**을 생성하여 사용자에게 반환합니다.  
3. 사용자는 이 **Pre-Signed URL**을 이용해 **Blob Storage에 직접 파일을 업로드**합니다.  
4. 업로드가 완료되면 사용자는 **업로드된 파일의 URL을 Chat Server에 전달**하여 메시지와 함께 저장합니다.  

#### 파일 다운로드 플로우
1. 사용자가 특정 첨부 파일을 다운로드하려고 하면 서버에 Pre-Signed URL을 요청합니다.  
2. **Chat Server**는 Blob Storage에서 해당 파일에 접근할 수 있는 **Pre-Signed URL을 반환**합니다.  
3. 사용자는 **해당 URL을 통해 직접 Blob Storage에서 파일을 다운로드**합니다.  

이상적으로는 모든 수신자가 파일을 다운로드한 후 자동 삭제하는 것이 가장 효율적이므로, 메시지 전송 후 수신자가 다운로드 했는지 확인하는 로직이 필요합니다. 또한 파일을 일정 기간 이후 자동 삭제하는 정책을 적용시킬 수도 있습니다.
## 상세 설계
### 1. 수십억 명의 유저가 동시 접속할 경우 어떻게 처리할 것인가?
위에서는 단일 서버를 가정했지만, **단일 서버로 수십억 명의 유저를 처리하는 것은 현실적으로 불가능**합니다.  
가장 직관적인 해결 방법은 **서버를 늘려서 트래픽을 분산하는 것(수평 확장, Horizontal Scaling)** 입니다.  

예를 들어, **전 세계적으로 10억 명의 유저가 있다면 2억 명이 동시 접속하는 것도 충분히 가능한 시나리오**입니다.  
그러나 단순히 서버를 늘리는 것만으로는 해결되지 않는 문제들도 존재합니다.  

먼저, **유저가 서로 다른 서버에 연결될 경우 메시지 전송이 불가능**해집니다. 예를 들어, A 유저가 서버 1에 연결되어 있고, B 유저가 서버 2에 연결되어 있다면 두 유저 간 메시지를 주고받기 위해서는 서버 간의 데이터 동기화가 필요해집니다.

이 문제를 해결하기 위해 **Redis Pub/Sub과 같은 메시지 브로커 시스템을 활용**할 수 있습니다.  
Redis는 **가벼운 해시맵(HashMap) 기반의 소켓 연결 관리 기능을 제공하여 메시지를 빠르게 라우팅**할 수 있습니다. 
 
#### Redis Pub/Sub 기반 메시지 전달 플로우
**메시지를 받을 때**
1. 사용자가 서버에 웹소켓을 연결합니다.
2. 서버는 Redis Pub/Sub에서 해당 유저 ID를 구독 (subscribe) 합니다.
3. 이후, 해당 유저에게 전달되는 메시지는 **구독된 Pub/Sub 채널을 통해 서버로 전달**됩니다.
4. 서버는 받은 메시지를 웹소켓을 통해 유저에게 전달합니다.

**메시지를 보낼 때**
1. 송신자가 메시지를 보내면 **서버는 수신자의 Pub/Sub 채널에 메시지를 Publish**합니다.  
2. 해당 메시지는 **수신자를 구독(Subscribe) 중인 모든 서버에서 수신**됩니다.  
3. 각 서버는 **수신자가 현재 연결된 상태인지 확인하고, 연결된 경우 웹소켓을 통해 메시지를 전달**합니다.  

여기서 Redis Pub/Sub의 한계도 존재합니다. Redis Pub/Sub은 **"At most once"** 전송 방식을 가지고 있는데, **구독자가 없을 경우 메시지는 손실**될 수 있습니다.

하지만, 우리는 이미 `Inbox` 테이블을 통해 메시지 내구성을 보장하고 있기 때문에 문제가 되지 않습니다.

그러나 수십억 명의 유저를 감당하려면 Redis Pub/Sub 자체도 확장 가능하게 설계해야 합니다.
Redis는 클러스터 모드(Redis Cluster)를 지원하지만, Pub/Sub 자체는 기본적으로 클러스터 샤딩을 지원하지 않습니다. 즉, 단순히 Redis Cluster를 활성화한다고 해서 Pub/Sub 메시지가 자동으로 여러 노드에 분산되지 않습니다.

따라서, **수동으로 유저 ID를 기준으로 특정 Redis 노드에 Pub/Sub 메시지를 라우팅하는 방식**을 적용해야 합니다.

#### Redis Cluster 기반 샤딩 적용 플로우
Redis Cluster는 데이터를 **키(Key) 값에 따라 여러 노드(Shard)로 분산 저장**하는 기능을 제공합니다.  
이러한 방식은 **Consistent Hashing**을 활용하여 **유저 ID를 기준으로 항상 동일한 노드에서 Pub/Sub 메시지를 처리할 수 있도록 보장**합니다.  

1. **유저 ID를 기반으로 특정 Redis 노드(Shard)를 할당**합니다. (Consistent Hashing 사용)  
2. 각 서버는 **특정 Redis 노드에서만 Pub/Sub 메시지를 Publish & Subscribe** 합니다.  
3. **메시지를 보내는 서버가 수신자의 Redis 노드를 찾아 Publish** 합니다.  
4. 수신자가 연결된 서버는 해당 Redis 노드에서 구독(Subscribe)한 후, 메시지를 전달합니다.  

### 2. 다중 기기 지원 문제
지금까지는 유저가 하나의 기기만 사용한다고 가정했습니다. 그러나 현실적으로 대부분의 유저는 여러 기기를 사용합니다.

예를 들어, 내 휴대폰에서는 메시지를 받았지만 노트북이 꺼져있었다면 노트북을 켰을 때 **누락된 메시지를 받아서 최신 상태로 동기화** 할 수 있어야 합니다. 
하지만 기존 **유저 단위로 메시지 전달을 추적**하는 `Inbox` 테이블 만으로는 이를 해결할 수 없습니다.

다중 기기 지원 시, 고려해야할 사항은 다음과 같습니다.
- 유저가 사용하는 모든 기기를 추적해야 합니다. 또한, 유저가 로그인하면 현재 활성화된 모든 기기를 관리할 방법이 필요합니다.
- 더 이상 사용되지 않는 기기를 자동으로 비활성화 해야합니다.
- 기기별로 메시지 전송을 관리해야합니다.

이를 해결하기 위해 기존 설계를 최대한 변경하지 않고 방법을 찾아보겠습니다.
#### 1. `Clients` 테이블 추가 (유저별 활성화된 기기 추적)
| **userId** | **clientId (device identifier)** | **lastActive** |
|-----------|-----------------------------------|----------------|
| user123   | phone_abc                         | 2025-02-20     |
| user123   | laptop_xyz                        | 2025-02-19     |
| user456   | tablet_def                        | 2025-02-18     |

#### 2. `Inbox` 테이블을 유저 단위가 아닌 "기기 단위"로 변경
각 기기가 개별적으로 메시지를 관리할 수 있으므로 기기 간 메시지 동기화 문제를 해결할 수 있고, 기기가 오프라인 상태였다가 다시 연결되었을 때 `Inbox` 테이블을 조회하여 전과 같은 방식으로 최신 메시지들을 받을 수 있습니다.

**변경 전 (유저 단위 Inbox)**
| **userId** | **messageId** | **status**  |
|-----------|-------------|----------|
| user123   | msg_001     | pending  |
| user123   | msg_002     | pending  |

**변경 후 (기기 단위 Inbox)**
| **clientId** | **messageId** | **status**  |
|--------------|---------------|-------------|
| phone_abc    | msg_001       | pending     |
| laptop_xyz   | msg_001       | pending     |
| phone_abc    | msg_002       | pending     |
| laptop_xyz   | msg_002       | pending     |

#### 3. Pub/Sub 구독 방식 변경
기존에는 유저 ID 기준으로 서버가 Pub/Sub을 구독했지만 이제는 기기 ID 기준으로 구독하도록 변경합니다.
또한, 기존에는 **유저 ID**를 기준으로 메시지를 보냈지만 이제는 **유저의 활성화된 모든 기기(Client)를 조회하여 각각 메시지를 전송**해야 합니다.

**메시지를 보낼 때**  
1. 송신자가 메시지를 보냅니다.
2. 서버는 **수신자의 `Clients` 테이블을 조회하여 활성화된 기기 목록**을 가져옵니다. 
3. **각 기기의 Pub/Sub 채널에 메시지를 Publish** 합니다.  
4. 해당 기기에 연결된 Chat Server가 메시지를 받아 웹소켓을 통해 전달합니다.  
5. **각 기기가 메시지를 받은 후 `ack`를 반환하면, 해당 기기의 `Inbox`에서 메시지를 삭제**합니다.  

## 마무리
오늘은 위와 같이 대규모 사용자를 대상으로 한 채팅 서비스를 디자인 해보았습니다. 다음 번엔 대규모 사용자를 위한 게시판 서비스를 디자인 해보겠습니다. 감사합니다.

### Reference
- https://redis.io/docs/latest/develop/interact/pubsub/
- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-indexes-general.html
- https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html
- https://youtu.be/cr6p0n0N-VA
7:["category","system-design","d"]
0:["mDy0wqT6GHTA5WnIIszLw",[[["",{"children":["blog",{"children":[["category","system-design","d"],{"children":["__PAGE__?{\"category\":\"system-design\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["category","system-design","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"category":"system-design","filteredPosts":[{"slug":"system-design/elasticsearch","categorySlug":"system-design","title":{"ko":"엘라스틱서치","en":"Elasticsearch"},"date":"2025-02-24 14:16","category":{"ko":"시스템 디자인","en":"System Design"},"description":{"ko":"엘라스틱서치의 기본 개념과 동작 원리","en":"Intro to Elasticsearch and how it works"},"content":"$3"},{"slug":"system-design/event-broker-vs-message-broker","categorySlug":"system-design","title":{"ko":"이벤트 브로커와 메시지 브로커","en":"Event Broker and Message Broker"},"date":"2025-02-23 18:11","category":{"ko":"시스템 디자인","en":"System Design"},"description":{"ko":"이벤트 브로커와 메시지 브로커의 차이, RabbitMQ와 Kafka의 동작 원리","en":"Difference between event broker and message broker, how RabbitMQ and Kafka work"},"content":"$4"},{"slug":"system-design/messaging-service","categorySlug":"system-design","title":{"ko":"메시징 서비스 시스템 디자인","en":"Design Messaging Service"},"date":"2025-02-20 00:00","category":{"ko":"시스템 디자인","en":"System Design"},"description":{"ko":"대규모 메시징 서비스 설계","en":"Let's design messaging service"},"content":"$5"}]}],null],null],null]},[null,["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$7","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[null,["$","$L9",null,{"children":["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}],"params":{}}]],null],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/julie/_next/static/css/064e10fa6619f508.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/julie/_next/static/css/e680cef9016abb97.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"ko","suppressHydrationWarning":true,"children":["$","body",null,{"className":"__className_29e2ff","children":["$","$La",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"disableTransitionOnChange":true,"children":["$","$Lb",null,{"children":[["$","$Lc",null,{}],["$","$L6",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L8",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]]}]}]}]}]],null],null],["$Ld",null]]]]
d:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Julie Lee's Portfolio"}],["$","meta","3",{"name":"description","content":"Welcome to Julie's portfolio page."}],["$","meta","4",{"name":"next-size-adjust"}]]
1:null
