2:I[3422,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","269","static/chunks/269-a28aad18182cd41e.js","254","static/chunks/app/blog/%5Bcategory%5D/page-6078f51c25971824.js"],"default"]
7:I[4707,[],""]
9:I[6423,[],""]
a:I[3483,["648","static/chunks/648-3ae006cfe07c9d94.js","768","static/chunks/app/blog/layout-a0ac16c7cad7b2d1.js"],"default",1]
b:I[5495,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","185","static/chunks/app/layout-2f9a78561536bd6f.js"],"ThemeProvider"]
c:I[4491,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","185","static/chunks/app/layout-2f9a78561536bd6f.js"],"LanguageProvider"]
d:I[1890,["137","static/chunks/137-7c01c277e0f0cc48.js","648","static/chunks/648-3ae006cfe07c9d94.js","185","static/chunks/app/layout-2f9a78561536bd6f.js"],"Header"]
3:T11c5,
지난 2주 (2025.02.15 - 2025.03.03) 동안 알고리즘 공부를 정말 열심히 했다.
체감상 예전에 처음으로 풀타임 취업 준비했을 때보다 훨씬 더 몰입해서 한 것 같다. 

![2월 기준](/julie/assets/til-004-20250304-2.png)
![3월 3일 기준](/julie/assets/til-004-20250304-1.png)

이 기간 동안 총 **254문제**를 풀었는데, **Easy 82문제, Medium 157문제, Hard 15문제**였다. 
중간에 복습 차원에서 두세 번 다시 푼 문제들도 있고, 처음 접하는 문제도 있었다.

이 때 사실 중간고사까지 겹쳐서(2월 말에 문제 조금만 푼 날들은 중간고사 대비한 날들) 스트레스가 꽤 심했다.
어떤 날은 아직 할게 많이 남았다는 불안한 마음에 휴가까지 써가면서 풀었다.

> 개인적으로 기회가 찾아왔을 때 노력하지 않아서 놓치는 상황을 견디기가 어렵ㄷㅏ .. 
차라리 최선을 다했는데도 안 됐다면 그냥 지금 당장은 나와 잘 맞지 않는 거라고 생각하고 넘길 수 있다.
이 블로그도 그런 의미에서, 내가 공부한 내용을 잊지 않고 나중에 다시 참고하려는 목적의 일환이다.

사실 학부생 때부터 알고리즘이 강한 편은 아니었는데, 다행히 100문제 정도 풀었을 때 취업을 해서 이후엔 1년에 몇십 문제 정도만 복습하면서 살았다. 
그런데 요즘 이직을 고민하면서 **"혹시 알고리즘이 발목을 잡으면 어떡하지?"** 하는 불안감이 커졌고, 그래서 이번에 집중적으로 공부했던 것 같다.

그래도 이제는 Easy/Medium 문제를 보면 처음 보는 유형이 아닌 이상 대충 감이 오고, 풀이 방법이 떠오르는 수준까지 왔다. (하지만 Hard 문제는 아직 어렵다 😂)

### 2주 Grinding에 도움이 되었던 방법

이 방법들은 어느 정도 알고리즘 / 데이터구조 사전 지식이 있을 때 통하는 방법이다!
없으면 나는 neetcode에서 처음부터 시간을 들여 공부하는 것을 추천한다..!

1. **Neetcode.io**
    
    Neetcode는 학부 때부터 자주 봤던 유튜브 채널인데, 이제는 웹사이트까지 생겨서 훨씬 체계적으로 정리되어 있다. 
    문제 설명이 엄청 깔끔하고 이해하기 쉬운데, 가끔 난해하게 설명되는 문제들이 있으면 **Greg Hogg 유튜브**를 참고했다.
    
    특히 **Roadmap** 기능이 있어서, 어떤 문제에 어떤 자료구조를 써야 할지 감이 잘 안 잡히는 사람에게 정말 유용하다. 
       
    알고리즘 문제들은 처음부터 최적화된 솔루션을 찾으려고 하면 오히려 더 어렵다. 
    (문제 by 문제 지만) 대부분의 문제는 **Brute force → 최적화 과정**을 거치면서 풀이를 발전시키는 게 더 좋은 접근이다.

2. **70 leetcode problems in 5+ hours (youtube 영상)**
    
    이 영상은 Leetcode 70문제를 5시간 30분짜리 영상 안에 담아둔 건데, 주말 이틀 동안 따라가면서 문제를 풀어봤다.
    
    실제로 다 풀어보려면 영상 시간의 두 배 정도 걸리는 느낌이다. 
    하지만 의지가 약하거나 집중력이 흐려질 때는 이런 영상이 꽤 도움이 된다. 
    그냥 **"같이 풀어보는 느낌"** 이 들면서 계속 문제를 풀게 된다.
    
3. **google docs에 정리**
    
    ![google doc](/julie/assets/til-004-20250304-3.png)
    
    문제를 풀 때마다 문제 이름, 난이도, 풀이 방법, 시간 복잡도를 정리했다.
    
    **굵게 표시된 문제 → 한 번에 못 풀었던 문제**이고,
    못 풀었던 문제들은 나중에 다시 돌아가서 한 줄 설명을 보고도 또 기억이 안 나면 또 풀기를 반복했다.
    
4. **google sheets에 정리**

    ![google sheets](/julie/assets/til-004-20250304-4.png)
    
    어느 정도 문제를 풀고 나면, 다시 풀어야겠다고 생각되는 문제들을 엑셀에 정리하고, 문제를 다시 풀면서 **더 간단한 한 줄 요약을 추가**했다.
    
    면접이나 코딩 테스트 직전에 Redo 체크된 문제들 위주로 복습했고, 체크가 안 되어 있어도, 1. 문제 이름 보고 풀이가 안 떠오르면 2. 한 줄 설명을 보고, 그래도 기억 안 나면 3. 다시 풀었다.

웃긴 건 이렇게 열심히 연습했는데, 실제 면접에서는 구현 문제가 나왔고 내 머리가 하얘졌다는 점...

그래도 그 전보단 훨씬 자신감이 생긴 것 같다! ;)
4:Tdaf,
회사에서 MongoDB의 full text search를 사용하면서 embedded document 관련 제약사항을 접하게 되었다.

우리가 사용하는 데이터의 일부는 이런식으로 저장되어 있는데
```
{
  "A": [
    { "type": "a", "value": "something" },
    { "type": "a", "value": "something else" },
    { "type": "b", "value": "something" }
  ]
}
```

단순히 `"value": "something"`을 검색(fuzzy search 등)을 수행할 때는 array index를 통해 처리할 수 있다.

근데 `"type": "a"`, `"value": "something"` 처럼 **구체적인 필드 조합**으로 검색하려면 **embedded document** index가 필요하다. ([**참조**](https://www.mongodb.com/docs/atlas/atlas-search/embedded-document/))

문제는 embedded document index를 생성하면 **array 안의 각 객체마다 하나의 문서가 인덱싱**된다는 점이었다.

Mongo Atlas에서는 샤드 하나당 최대 21억(2.1 billion) 개의 embedded 문서를 인덱싱할 수 있다.
그런데 우리는 이미 2억개 이상의 문서가 단일 샤드에 들어 있는 상황이었고, 각 문서의 여러 필드에는 많개는 수백개의 객체가 포함될 수 있으므로, 
이 배열을 전부 embedded doc으로 인덱싱하다 보면 금세 21억 개를 초과하는 상황이었다.

그래서 다음과 같은 세 가지 해결책을 고민해봤다.

1. **스키마 변경**

    배열 안에 있는 key-value 구조를 최상위로 옮겨서 embedded document index를 사용하지 않아도 되도록 재설계한다.
    
    예를 들어, 기존에 `A: [{type: "...", value: "..."}]` 형태로 저장하던 문서를 `"a": ["something", "something else"]` 요런식으로 분리해서 일반 문자열 필드나 배열 인덱스로 검색 가능하게 만든다.
    
2. **Full text search 대신 element search**
    
    type과 value를 따로 인덱싱하고, wildcard나 Regex 같은 방식을 사용해서 검색한다.
    
    근데 이 방법은 **데이터들이 매우 일관적(standardized)으로 저장되어 있어야 동작**한다는 단점이 있다. (전처리 로직이 필수)
    
    그리고 score 개념이 없어서 어떤 문서가 더 관련이 높은지 판별하기도 어렵다.
    
3. **샤딩**

    샤드 수를 늘려서 샤드마다 embedded 문서 개수가 21억을 넘지 않게 분산한다.
    
    이 방법이 처음엔 가장 간단해 보였으나.. 이미 있는 데이터를 옮겨야하고, 샤딩 키를 어떻게 설정할지 등 여러 가지를 면밀히 고려해야한다.
    
    또한, 검색에 샤드 키 조건이 포함되지 않은 쿼리들은 모든 샤드에 broadcast 되고, 각 샤드에서 병렬 검색 후 결과를 취합해야 한다. (최종 응답 도출 시간/오버헤드 증가)
    
    즉, 샤드 관리 비용도 포함해서 비용이 더 많이 든다. ([**참조**](https://www.mongodb.com/resources/products/capabilities/database-sharding-explained#:~:text=Sharding%20does%20come%20with%20several,operation%20to%20the%20appropriate%20shard.))

결국 여러 가지 제약 상황(시간, 비용, 인력 자원 등등)으로 인해 우리는 **첫 번째 방식(데이터 스키마 재설계)** 을 선택했다.

이 방법도 엄청난 단점이 있었는데, 이미 있는 데이터들을 새로운 스키마로 다 옮겨야 한다는 것이었다..

그리고 이미 원래 스키마에 맞게 설정된 기존 코드들을 새로운 스키마에 맞게 전부 다 고쳐야 한다. :)


5:T156c,
데이터 파이프라인에서는 여러 서비스가 각각 다른 역할을 수행하는데, 때때로 **전체 흐름을 거치지 않고 특정 서비스의 결과만 확인**하고 싶을 때가 있다.

예를 들어, A 서비스에서 데이터를 변형하고 B 서비스가 그 데이터를 데이터베이스에 넣는다고 가정해보자. 이때 B 서비스에서 데이터베이스 저장 과정을 생략하고 A 서비스의 결과값을 확인하는 방법은 여러 가지가 있다.

두 서비스가 어떻게 연결되어 있느냐에 따라 방법이 달라질 수 있지만, 만일 RabbitMQ로 연결되어 있다면 크게 이런 방법들이 떠오른다:

 1. **메시지 자체에 flag를 추가**하고 로그에서 결과값을 확인한다. 
    
    메시지에 특정 flag가 있으면 다른 서비스들에서는 해당 메시지를 무시하도록 설정하는 방식이다.
    하지만 이 방법은 다른 서비스들이 메시지를 실제로 읽어야 하므로 메시지 전송량이 증가하고, 결과값을 확인하려면 로그에서 해당 서비스에 대한 메시지를 뒤져야 한다는 단점이 있다.
  
 2. RabbitMQ의 **temporary queue**를 사용한다. 
 
    클라이언트가 연결되어있는 동안만 유지되는 임시 큐를 활용한다.
    API 요청이 들어올 때마다 새로운 임시 큐를 생성하고, A 서비는 해당 큐에 결과값을 보낸다. 
    클라이언트는 이 큐에서 데이터를 읽은 후, 임시 큐를 삭제해서 불필요한 데이터가 쌓이는 걸 막을 수 있다.
 
 
오늘은 2번 방법에 대해 간단히 얘기를 해보겠다.

이 방법을 쓰면 API 요청으로 A 서비스 결과값을 즉시 확인하기가 훨씬 수월해진다.

RabbitMQ에서 temporary queue를 만들 때는 **`exclusive=True`나 `auto_delete=True`** 같은 옵션을 사용할 수 있는데

`exclusive=True`는 **한 connection에서만 접근 가능한 큐**가 되고, 그 connection(혹은 consumer)이 끊어지면 큐가 자동으로 제거된다.

`auto_delete=True`는 구독중인 소비자 수가 0이 되면 큐가 제거되는데 여러 consumer가 연결해도 문제가 없다.

그리고 아래 코드에서 콜백 함수가 한 개의 `Future`만 사용하기 때문에 동시에 하나의 응답만 받을 수 있다.
그래서 동시에 여러 응답을 처리해야 하는 상황이면, **publish 시점에 메세지마다 correlation_id를 지정**해 주고, `on_response` 콜백 함수에서 `msg.properties.correlation_id` 값을 확인해 **각 `Future`에 매핑**해줘야 한다.
(미리 dictionary를 만들어서 `key: correlation_id, value: Future` 식으로 관리)

또한, A 서비스가 제대로 동작하지 않거나, 이미 다루고 있는 메시지 양이 많아 늦을 경우를 대비해 Timeout을 설정해두었다.

아래 코드에서는 요청마다 새로운 큐를 생성하는 구조라서 동시에 많은 요청이 들어오면 큐가 너무 많이 생길 수 있다.
나는 한 번에 많은 양의 요청을 고려하지 않아도 되었었는데, 만약 요청량이 많은 상황이라면 **큐 풀(pool)** 을 구성하거나 하나의 큐를 공유하면서 메세지마다 `correlation id`로 구분하면 될 것 같다.

특히 대규모 트래픽에서는:

1. API 요청을 비동기로 처리해 DB에 저장 후 요청 ID를 반환하고, 처리 완료 시 결과를 DB나 캐시에 저장해 별도의 endpoint로 조회
2. 동시에 생성될 수 있는 최대 큐 개수를 제한하고, 요청들을 큐에 분산시켜 처리한다.

이렇게 처리할 수 있지 않을까.. 하는 생각을 했다.

```python
# 1. temporary queue 선언: queue=""와 exclusive=True를 지정하면
#    서버가 고유한 큐 이름을 자동으로 생성해주고,
#    연결이 끊기면 해당 큐가 사라진다.
reply_queue = await rabbit.queue_declare(queue="", exclusive=True)
reply_queue_name = reply_queue.queue or ""

# 2. 응답을 비동기로 기다리기 위한 Future 생성
#    (아직 완료되지 않은 작업 결과를 담아둘 그릇이라고 보면 됨)
loop = asyncio.get_running_loop()
response_future = loop.create_future()

# 3. on_response 콜백 함수: 임시 큐에서 메시지를 받으면 처리
async def on_response(msg: aiormq.abc.DeliveredMessage):
    if not response_future.done():
        response_body = msg.body.decode()
        res_data = json.loads(response_body)
        response_future.set_result(res_data)

    await rabbit.basic_ack(msg.delivery.delivery_tag)

# 4. 임시 큐 구독 시작: on_response로 메시지를 처리하도록 설정
consume_ok = await rabbit.basic_consume(reply_queue_name, on_response, no_ack=False)
consumer_tag = consume_ok.consumer_tag

# 5. 메시지 발행: reply_to에 임시 큐 이름을 넣어둬야
#    응답 메시지가 이 큐로 돌아온다.
await rabbit.basic_publish(
    message_body,
    routing_key=routing_key,
    exchange=exchange,
    properties=aiormq.spec.Basic.Properties(
        reply_to=reply_queue_name,
        content_type="application/json",
    ),
)

# 6. 응답을 최대 N초간 기다림
try:
    response_data = await asyncio.wait_for(response_future, timeout=N)
except asyncio.TimeoutError:
    response_data = {"error": "Timed out waiting for response."}

# 7. 소비자 취소 및 임시 큐 삭제
await rabbit.basic_cancel(consumer_tag)
await rabbit.queue_delete(reply_queue_name)
```
6:Tdd6,
Mongo Atlas Search에서 full-text search를 사용할 때 여러 search node로 쿼리를 분산시킬 수 있다.
그런데 쿼리를 실행할 때마다 **어느 search node를 사용하냐에 따라 결과가 달라진다**는 사실을 발견했다.

특히 search score가 낮은 document들만 꾸준히 드랍되는 것을 목격할 수 있었다.
구체적으로, query를 돌릴 때 각 search node가 몇 개의 문서를 훑어보고 결과를 도출하는지 확인할 수 있는데, 그 숫자가 검색할 때마다 달라지는 현상이 있었다. (따라서 검색 결과도 달라졌다.)


Mongo support 팀에 문의한 결과 이런 경우가 가끔 발생한다고 응답이 왔고, 아직 정확한 원인을 파악하지 못했다고 한다.

대신, search node(`mongot`)를 전용 노드에서 따로 실행하는 대신 **데이터베이스 자체(`mongod`)와 같은 노드에서 실행**하면 일관성있는 결과를 얻을 수 있을 것이라는 답변을 받았다.

하지만 서포트 팀에서 얘기한 솔루션은 [공식 문서](https://www.mongodb.com/docs/atlas/atlas-search/about/deployment-options/#fts-deployment-options)에 따르면 Testing/Prototyping 환경에 적합한 것으로, 이 사용 사례는 우리가 처한 상황과는 맞지 않다. 
우리가 인덱스 해야하는 총 문서는 2억 건 이상이고, 데이터 양도 당연히 10GB는 훨씬 넘기 때문이다.

그리고 `mongot`와 `mongod`가 같은 노드에서 실행되면 두 프로세스 간 리소스 contention이 발생할 수 있어 이상적인 환경은 아닌 것 같다.

Elasticsearch처럼 `mongot`도 Apache Lucene을 기반으로 만들어져있는데, [공식 문서](https://www.mongodb.com/docs/atlas/atlas-search/atlas-search-overview/#fts-architecture)에 따르면 세 가지 일을 담당한다.

1. Atlas search 인덱스 생성
2. 문서 상태 및 인덱스 변경 사항 모니터링
3. 검색 쿼리 처리: 여기서 문서 ID와 검색 점수를 산출하고, 이 내용을 바탕으로 `mongod`에서 최종 결과를 가져온다

처음엔 2번 과정에서 동기화 문제가 있어서 그런건가 싶었는데, 데이터베이스에 아무 변화가 없고 인덱스 rebuilding도 모두 완료된 상태에서도 검색 결과가 계속 달라지는 현상이 나타났다.
(eventual consistency와는 관련이 없는 상황)

그래서 지금 생각으로는 뭔가 인덱스에 있는 fuzzy search 때문에 Mongo 백그라운드에서 특정 document들을 제거 하는게 아닌가 싶긴 하지만, Mongo search 팀에서 일하는 사람이 원인을 모른다고 하니까 조금 답답하다.
(하지만 우리보다 훨씬 복잡한 인덱스를 쓰는 사람들도 별 문제가 없는 사람들도 많았다고 하니 확실하진 않다.)

우선 Mongo support에서 제시한 솔루션대로 내일 진행해보고, 쿼리 성능이나 결과를 좀 더 면밀히 분석한 후 추가로 글을 업데이트할 예정이다.

---
2025-02-28

업데이트: Resource contention 문제로 인해 단일 search node를 분리해서 사용하는 방안을 고민했으나, search query가 많을 경우 노드 하나로는 부하가 과도할 것으로 판단하였다. 
따라서 replica와 main DB에 **colocated search node**를 함께 구성하는 것이 최선이라는 결론을 내렸다.

`preference=secondary` 설정을 적용하여 replica가 모두 사용 불가능한 상황이 아닐 경우, 우선적으로 replica를 활용하도록 설정하였다.
8:["category","til","d"]
0:["nBlwvcTFiqFjWAR0inr3E",[[["",{"children":["blog",{"children":[["category","til","d"],{"children":["__PAGE__?{\"category\":\"til\"}",{}]}]}]},"$undefined","$undefined",true],["",{"children":["blog",{"children":[["category","til","d"],{"children":["__PAGE__",{},[["$L1",["$","$L2",null,{"category":"til","filteredPosts":[{"slug":"til/2025-03-04","categorySlug":"til","title":{"ko":"2025-03-04","en":"2025-03-04"},"date":"2025-03-04 17:01","category":{"ko":"TIL","en":"TIL"},"description":{"ko":"2주 동안의 알고리즘 공부 회고","en":"Reflections on grinding leetcode for 2 weeks"},"content":"$3"},{"slug":"til/2025-02-28","categorySlug":"til","title":{"ko":"2025-02-28","en":"2025-02-28"},"date":"2025-02-28 18:56","category":{"ko":"TIL","en":"TIL"},"description":{"ko":"MongoDB full-text search를 사용할 때의 임베디드 문서 관련 제한 사항","en":"Limitations of MongoDB full-text search on embedded documents"},"content":"$4"},{"slug":"til/2025-02-26","categorySlug":"til","title":{"ko":"2025-02-26","en":"2025-02-26"},"date":"2025-02-26 16:22","category":{"ko":"TIL","en":"TIL"},"description":{"ko":"RabbitMQ Temporary queue 사용법","en":"How and when to use rabbitMQ temporary queues"},"content":"$5"},{"slug":"til/2025-02-25","categorySlug":"til","title":{"ko":"2025-02-25","en":"2025-02-25"},"date":"2025-02-25 16:57","category":{"ko":"TIL","en":"TIL"},"description":{"ko":"Mongo Atlas full-text search에서 search node가 여러 개일 때 발생하는 문제","en":"Issues with multiple search nodes in Mongo Atlas full-text search"},"content":"$6"}]}],null],null],null]},[null,["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children","$8","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}]],null]},[[null,["$","$La",null,{"children":["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children","blog","children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","notFoundStyles":"$undefined"}],"params":{}}]],null],null]},[[[["$","link","0",{"rel":"stylesheet","href":"/julie/_next/static/css/064e10fa6619f508.css","precedence":"next","crossOrigin":"$undefined"}],["$","link","1",{"rel":"stylesheet","href":"/julie/_next/static/css/e680cef9016abb97.css","precedence":"next","crossOrigin":"$undefined"}]],["$","html",null,{"lang":"ko","suppressHydrationWarning":true,"children":["$","body",null,{"className":"__className_29e2ff","children":["$","$Lb",null,{"attribute":"class","defaultTheme":"system","enableSystem":true,"disableTransitionOnChange":true,"children":["$","$Lc",null,{"children":[["$","$Ld",null,{}],["$","$L7",null,{"parallelRouterKey":"children","segmentPath":["children"],"error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L9",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":"404"}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],"notFoundStyles":[]}]]}]}]}]}]],null],null],["$Le",null]]]]
e:[["$","meta","0",{"name":"viewport","content":"width=device-width, initial-scale=1"}],["$","meta","1",{"charSet":"utf-8"}],["$","title","2",{"children":"Julie Lee's Portfolio"}],["$","meta","3",{"name":"description","content":"Welcome to Julie's portfolio page."}],["$","meta","4",{"name":"next-size-adjust"}]]
1:null
